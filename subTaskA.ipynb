{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('SemEval2017-task4-dev.subtask-A.english.INPUT.txt', names = ['id', 'label', 'content'], sep ='\\t', index_col=False)\n",
    "data_bu = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619950566786113536</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Picturehouse's, Pink Floyd's, 'Roger Waters: T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>619969366986235905</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Order Go Set a Watchman in store or through ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>619971047195045888</td>\n",
       "      <td>negative</td>\n",
       "      <td>If these runway renovations at the airport pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>619974445185302528</td>\n",
       "      <td>neutral</td>\n",
       "      <td>If you could ask an onstage interview question...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>619987808317407232</td>\n",
       "      <td>positive</td>\n",
       "      <td>A portion of book sales from our Harper Lee/Go...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id     label  \\\n",
       "0  619950566786113536   neutral   \n",
       "1  619969366986235905   neutral   \n",
       "2  619971047195045888  negative   \n",
       "3  619974445185302528   neutral   \n",
       "4  619987808317407232  positive   \n",
       "\n",
       "                                             content  \n",
       "0  Picturehouse's, Pink Floyd's, 'Roger Waters: T...  \n",
       "1  Order Go Set a Watchman in store or through ou...  \n",
       "2  If these runway renovations at the airport pre...  \n",
       "3  If you could ask an onstage interview question...  \n",
       "4  A portion of book sales from our Harper Lee/Go...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stop words, after tokenization\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "stop_words.remove('not')\n",
    "snowBallStemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "#steming\n",
    "#tokenized_df = pd.DataFrame(columns=['id', 'label',])\n",
    "# for i in range(data.shape[0]):\n",
    "#     sample = data.loc[i]['content'].lower()\n",
    "#     token1 = word_tokenize(sample)\n",
    "#     token2 =[]\n",
    "#     for tk in token1:\n",
    "#         if tk in stop_words:\n",
    "#             token1.remove(tk)\n",
    "#         else:\n",
    "#             token2.append(re.sub('[^a-zA-Z\\s]', '', tk))\n",
    "#     token2 = list(filter(lambda a: a not in ['', 's'], token2))\n",
    "#     stemWords = [snowBallStemmer.stem(word) for word in token2]\n",
    "#     data.at[i, 'content'] = stemWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ps = pos_tag(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete_list = ['NNP', 'PRP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n"
     ]
    }
   ],
   "source": [
    "# nltk.help.upenn_tagset('PRP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data = pd.read_csv('SemEval2017-task4-dev.subtask-A.english.INPUT.txt', names = ['id', 'label', 'content'], sep ='\\t', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     10342\n",
       "positive     7059\n",
       "negative     3231\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Sentiment_count=data.groupby('label').count()\n",
    "plt.bar(Sentiment_count.index.values, Sentiment_count['content'])\n",
    "plt.xlabel('Twitt Sentiments')\n",
    "plt.ylabel('Number of Twitts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre process : remove delete list\n",
    "# delete_list = ['NNP', 'PRP']\n",
    "# for d in range(data.shape[0]):\n",
    "#     #print(d)\n",
    "#     sen = ''\n",
    "#     r = word_tokenize(data.loc[d]['content'])\n",
    "#     ps = pos_tag(r)\n",
    "#     for i in ps:\n",
    "#     #print(d)\n",
    "#         if i[1] not in delete_list:\n",
    "#             sen += i[0] + ' '\n",
    "#     data.at[d, 'content'] = sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#tokenizer to remove unwanted elements from out data like symbols and numbers\n",
    "token = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,4),tokenizer = token.tokenize)\n",
    "text_counts= cv.fit_transform(data['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_counts, data['label'], test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy: 0.5959612277867529\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "# Model Generation Using Multinomial Naive Bayes\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(min_samples_split=20, random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = result.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT Accuracy: 0.5350565428109855\n"
     ]
    }
   ],
   "source": [
    "print(\"DT Accuracy:\",metrics.accuracy_score(y_test, pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7043345796981028"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5610662358642973"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5610662358642973\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test, pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df = pd.DataFrame(columns=['id', 'label', 'content'])\n",
    "word_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stop words, after tokenization\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "snowBallStemmer = SnowballStemmer(\"english\")\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk import pos_tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(data.shape[0]):\n",
    "    sample = data.loc[i]['content']\n",
    "    token1 = word_tokenize(sample)\n",
    "#     token2 =[]\n",
    "#     for tk in token1:\n",
    "#         if tk in stop_words:\n",
    "#             token1.remove(tk)\n",
    "#         else:\n",
    "#             token2.append(re.sub('[^a-zA-Z\\s]', '', tk))\n",
    "#     token2 = list(filter(lambda a: a not in ['', 's'], token2))\n",
    "#     stemWords = [snowBallStemmer.stem(word) for word in token2]\n",
    "    token2 = [lemmatizer.lemmatize(x) for x in token1]\n",
    "    ps_tag = pos_tag(token2)\n",
    "    token3 = []\n",
    "    for d in ps_tag:\n",
    "        d = list(d)\n",
    "        if d[1] == 'NN':\n",
    "            d[0] = 'NAME'\n",
    "        token3.append(d[0])\n",
    "    \n",
    "    token_df.at[i] = [data.loc[i]['id'], data.loc[i]['label'],token3]\n",
    "    word_list += token3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creat dictionary of words:\n",
    "from collections import Counter\n",
    "## Build a dictionary that maps words to integers \n",
    "counts = Counter(word_list)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word_list: ii for ii, word_list in enumerate(vocab, 1)}\n",
    "vocab_df = pd.DataFrame(columns=['id', 'label', 'content'])\n",
    "for i in range(token_df.shape[0]):\n",
    "    temp = []\n",
    "    for j in token_df.loc[i]['content']:\n",
    "        temp.append(vocab_to_int[j])\n",
    "    vocab_df.at[i] = [token_df.loc[i]['id'], token_df.loc[i]['label'],temp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(vocab_df['content'], key = len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30453"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "padd_data = sequence.pad_sequences(vocab_df['content'], maxlen=54)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = token_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = np.zeros(20632)\n",
    "for l in range(labels.shape[0]):\n",
    "    if labels[l] == 'neutral':\n",
    "        encoded_labels[l] = 0\n",
    "    elif labels[l] == 'positive':\n",
    "        encoded_labels[l] = 1\n",
    "    elif labels[l] == 'negative':\n",
    "        encoded_labels[l] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 54, 32)            974528    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 1,028,031\n",
      "Trainable params: 1,028,031\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#test train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "embedding_size=32\n",
    "model=Sequential()\n",
    "model.add(Embedding(30454, embedding_size, input_length=54))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14378 samples, validate on 64 samples\n",
      "Epoch 1/3\n",
      "14378/14378 [==============================] - 29s 2ms/sample - loss: 0.9766 - accuracy: 0.5214 - val_loss: 0.8125 - val_accuracy: 0.5938\n",
      "Epoch 2/3\n",
      "14378/14378 [==============================] - 24s 2ms/sample - loss: 0.7462 - accuracy: 0.6685 - val_loss: 0.7563 - val_accuracy: 0.6875\n",
      "Epoch 3/3\n",
      "14378/14378 [==============================] - 24s 2ms/sample - loss: 0.5410 - accuracy: 0.7826 - val_loss: 0.7816 - val_accuracy: 0.6719\n",
      "Test accuracy: 0.623748\n",
      "Train on 14378 samples, validate on 64 samples\n",
      "Epoch 1/3\n",
      "14378/14378 [==============================] - 24s 2ms/sample - loss: 0.3813 - accuracy: 0.8539 - val_loss: 0.7929 - val_accuracy: 0.7344\n",
      "Epoch 2/3\n",
      "14378/14378 [==============================] - 24s 2ms/sample - loss: 0.2702 - accuracy: 0.9028 - val_loss: 0.9514 - val_accuracy: 0.6406\n",
      "Epoch 3/3\n",
      "14378/14378 [==============================] - 24s 2ms/sample - loss: 0.2119 - accuracy: 0.9234 - val_loss: 1.0747 - val_accuracy: 0.6875\n",
      "Test accuracy: 0.6161551\n",
      "Train on 14378 samples, validate on 64 samples\n",
      "Epoch 1/3\n",
      "14378/14378 [==============================] - 27s 2ms/sample - loss: 0.1594 - accuracy: 0.9446 - val_loss: 0.9992 - val_accuracy: 0.6875\n",
      "Epoch 2/3\n",
      "14378/14378 [==============================] - 26s 2ms/sample - loss: 0.1489 - accuracy: 0.9475 - val_loss: 1.3051 - val_accuracy: 0.6562\n",
      "Epoch 3/3\n",
      "14378/14378 [==============================] - 26s 2ms/sample - loss: 0.1215 - accuracy: 0.9569 - val_loss: 1.2992 - val_accuracy: 0.5938\n",
      "Test accuracy: 0.6003231\n",
      "Train on 14378 samples, validate on 64 samples\n",
      "Epoch 1/3\n",
      "14378/14378 [==============================] - 26s 2ms/sample - loss: 0.1018 - accuracy: 0.9634 - val_loss: 1.3851 - val_accuracy: 0.6562\n",
      "Epoch 2/3\n",
      "14378/14378 [==============================] - 27s 2ms/sample - loss: 0.0884 - accuracy: 0.9683 - val_loss: 1.3591 - val_accuracy: 0.6562\n",
      "Epoch 3/3\n",
      "14378/14378 [==============================] - ETA: 0s - loss: 0.0804 - accuracy: 0.97 - 27s 2ms/sample - loss: 0.0808 - accuracy: 0.9709 - val_loss: 1.6582 - val_accuracy: 0.6406\n",
      "Test accuracy: 0.58966076\n",
      "Train on 14378 samples, validate on 64 samples\n",
      "Epoch 1/3\n",
      "14378/14378 [==============================] - 28s 2ms/sample - loss: 0.0750 - accuracy: 0.9732 - val_loss: 1.1794 - val_accuracy: 0.7031\n",
      "Epoch 2/3\n",
      "14378/14378 [==============================] - 28s 2ms/sample - loss: 0.0676 - accuracy: 0.9752 - val_loss: 1.3038 - val_accuracy: 0.7188\n",
      "Epoch 3/3\n",
      "14378/14378 [==============================] - 29s 2ms/sample - loss: 0.0669 - accuracy: 0.9769 - val_loss: 1.4601 - val_accuracy: 0.6875\n",
      "Test accuracy: 0.598546\n",
      "Train on 14378 samples, validate on 64 samples\n",
      "Epoch 1/3\n",
      "14378/14378 [==============================] - 29s 2ms/sample - loss: 0.0606 - accuracy: 0.9795 - val_loss: 1.5437 - val_accuracy: 0.6406\n",
      "Epoch 2/3\n",
      "14378/14378 [==============================] - 30s 2ms/sample - loss: 0.0552 - accuracy: 0.9800 - val_loss: 1.3451 - val_accuracy: 0.6719\n",
      "Epoch 3/3\n",
      "14378/14378 [==============================] - 30s 2ms/sample - loss: 0.0521 - accuracy: 0.9816 - val_loss: 1.4276 - val_accuracy: 0.6719\n",
      "Test accuracy: 0.5848142\n",
      "Train on 14378 samples, validate on 64 samples\n",
      "Epoch 1/3\n",
      "14378/14378 [==============================] - 32s 2ms/sample - loss: 0.0442 - accuracy: 0.9848 - val_loss: 1.7455 - val_accuracy: 0.6406\n",
      "Epoch 2/3\n",
      "14378/14378 [==============================] - 33s 2ms/sample - loss: 0.0415 - accuracy: 0.9854 - val_loss: 1.6047 - val_accuracy: 0.6562\n",
      "Epoch 3/3\n",
      "14378/14378 [==============================] - 33s 2ms/sample - loss: 0.0368 - accuracy: 0.9876 - val_loss: 1.5506 - val_accuracy: 0.6719\n",
      "Test accuracy: 0.58093697\n",
      "Train on 14378 samples, validate on 64 samples\n",
      "Epoch 1/3\n",
      "14378/14378 [==============================] - 36s 2ms/sample - loss: 0.0372 - accuracy: 0.9865 - val_loss: 2.0279 - val_accuracy: 0.6406\n",
      "Epoch 2/3\n",
      "14378/14378 [==============================] - 37s 3ms/sample - loss: 0.0335 - accuracy: 0.9889 - val_loss: 1.6448 - val_accuracy: 0.6562\n",
      "Epoch 3/3\n",
      "14378/14378 [==============================] - 36s 2ms/sample - loss: 0.0275 - accuracy: 0.9904 - val_loss: 2.0134 - val_accuracy: 0.6094\n",
      "Test accuracy: 0.5936995\n",
      "Train on 14378 samples, validate on 64 samples\n",
      "Epoch 1/3\n",
      "14378/14378 [==============================] - 37s 3ms/sample - loss: 0.0278 - accuracy: 0.9901 - val_loss: 1.7228 - val_accuracy: 0.6562\n",
      "Epoch 2/3\n",
      "14378/14378 [==============================] - 38s 3ms/sample - loss: 0.0238 - accuracy: 0.9916 - val_loss: 1.6011 - val_accuracy: 0.6250\n",
      "Epoch 3/3\n",
      "14378/14378 [==============================] - 38s 3ms/sample - loss: 0.0239 - accuracy: 0.9914 - val_loss: 2.1496 - val_accuracy: 0.6250\n",
      "Test accuracy: 0.57576734\n",
      "Train on 14378 samples, validate on 64 samples\n",
      "Epoch 1/3\n",
      "14378/14378 [==============================] - 40s 3ms/sample - loss: 0.0296 - accuracy: 0.9894 - val_loss: 1.6884 - val_accuracy: 0.6250\n",
      "Epoch 2/3\n",
      "14378/14378 [==============================] - 41s 3ms/sample - loss: 0.0301 - accuracy: 0.9891 - val_loss: 1.7596 - val_accuracy: 0.6719- ETA: 20s - loss: 0.0 - ETA: 16s - loss: 0.0264 - accuracy: 0.9 - ETA: 16s - loss: 0. - ETA - ETA: 3s - l - ETA: 0s - loss: 0.0303 - accuracy: 0.98 - ETA: 0s - loss: 0.0303 - accuracy: 0.98 - ETA: 0s - loss: 0.0302 - accuracy: 0.\n",
      "Epoch 3/3\n",
      "14378/14378 [==============================] - 42s 3ms/sample - loss: 0.0226 - accuracy: 0.9910 - val_loss: 1.6173 - val_accuracy: 0.6719ETA: 28s - loss: - E - ETA: 8s - loss: 0 - ETA: 6s - l\n",
      "Test accuracy: 0.57253635\n",
      "final accuracy:\t 0.5936187326908111\n"
     ]
    }
   ],
   "source": [
    "final_accuracy = 0\n",
    "for r in range(10):\n",
    "    batch_size = 64\n",
    "    num_epochs = 3\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    padd_data, encoded_labels, test_size=0.3, random_state=100)\n",
    "    X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\n",
    "    X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]\n",
    "    model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test accuracy:', scores[1])\n",
    "    final_accuracy += scores[1]\n",
    "\n",
    "print('final accuracy:\\t', final_accuracy/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619950566786113536</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Picturehouse's, Pink Floyd's, 'Roger Waters: T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>619969366986235905</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Order Go Set a Watchman in store or through ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>619971047195045888</td>\n",
       "      <td>negative</td>\n",
       "      <td>If these runway renovations at the airport pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>619974445185302528</td>\n",
       "      <td>neutral</td>\n",
       "      <td>If you could ask an onstage interview question...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>619987808317407232</td>\n",
       "      <td>positive</td>\n",
       "      <td>A portion of book sales from our Harper Lee/Go...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id     label  \\\n",
       "0  619950566786113536   neutral   \n",
       "1  619969366986235905   neutral   \n",
       "2  619971047195045888  negative   \n",
       "3  619974445185302528   neutral   \n",
       "4  619987808317407232  positive   \n",
       "\n",
       "                                             content  \n",
       "0  Picturehouse's, Pink Floyd's, 'Roger Waters: T...  \n",
       "1  Order Go Set a Watchman in store or through ou...  \n",
       "2  If these runway renovations at the airport pre...  \n",
       "3  If you could ask an onstage interview question...  \n",
       "4  A portion of book sales from our Harper Lee/Go...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#tokenizer to remove unwanted elements from out data like symbols and numbers\n",
    "token = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,4),tokenizer = token.tokenize)\n",
    "text_counts= cv.fit_transform(data['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = np.zeros(20632)\n",
    "for l in range(data.shape[0]):\n",
    "    if data.loc[l].label == 'neutral':\n",
    "        encoded_labels[l] = 0\n",
    "    elif data.loc[l].label == 'positive':\n",
    "        encoded_labels[l] = 1\n",
    "    elif data.loc[l].label == 'negative':\n",
    "        encoded_labels[l] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data.iloc[:,:-1],data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_counts, encoded_labels, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = xgb.XGBClassifier(objective ='objective=multi:softmax', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 10, alpha = 10, n_estimators = 40)\n",
    "xg_reg.fit(X_train,y_train)\n",
    "\n",
    "preds = xg_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.888502\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6081899685001212\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6081899685001212"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
